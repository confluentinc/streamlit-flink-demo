from .script import SqlScript
from .files import dump_delimited, dump_json
from .log import get_logger
from api.statements import StatementsEndpoint
from api.auth import AuthEndpoint
from confluent_kafka.admin import AdminClient
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.error import SchemaRegistryError
import functools
import os
import time
import sys


log = get_logger()


# Simple class extending list that encapsulates a table generated from a
# changelog. Tables support incremental updates so that callers don't have
# to consume an entire changelog each time they want to update a table.
#
# The pattern is to use the set of new changelog records returned by consume(),
# and then pass those to update(), like so:
#
#   log = Changelog(schema, rows)
#   log.consume(n)
#   table = log.collapse()
#
#   new = log.consume(n)
#   table.update(new)
#
# At this point the table will be equivalent to a table generated by scanning all
# 2 * n records in the changelog, but will not have performed any extraneous scans
# in order to reflect the new changelog records.
class Table(list):

  def __init__(self, columns):
    self.columns = columns

  def update(self, rows):
    def remove(r):
      if r not in self:
        log.warning('no corresponding row in table to remove: %s', r)
        return
      self.remove(r)

    handlers = {
      '+I': self.append,
      '-U': remove,
      '+U': self.append,
      '-D': remove,
      None: self.append
    }
    for row in rows:
      op, row = row[0], row[1:]
      # Changelog should have already validated that these changelog records
      # contain valid op codes
      handlers[op](row)


# Lightweight wrapper around result sets returned by the API. Encapsultes
# things that are useful when dealing with result sets, such as statement
# start time (latency), schema, rows, and functionality for interpreting its
# underlying changelog.

# Callers may retrieve the raw changelog consumed thus far, which is serialized
# as a list. Each changelog element is a tuple of the form:
#
#   <op code, column1, column2, ...>
#
# Callers may also collapse the changelog consumed thus far, which will scan
# the changelog and apply inserts, updates and deletes as necessary. Tables
# may be obtained by calling the collapse() method. They have the following form:
#
#    <column1, column2, ...>
#
# These table rows are returned within a Table instance.
#
# Changelogs also inherit all in-context SET properties from when their
# corresponding statements were executed by the interpreter, giving users
# control over how they are displayed.
class Changelog(object):

  def __init__(self, schema, rows, statement=None, ts=None, properties=None):
    # It's easier to always work with generators instead of having to special
    # case arrays. If it's already a generator this is a noop. Wrapping changelog
    # records in an iterator is also good for correctness, as it ensures that this
    # changelog's history can only be scanned sequentially. If callers want to work
    # with changelog records more freely, they can simply wrap the changelog in a list.
    self.rows = iter(rows)
    self.statement = statement or {}
    self.properties = properties or {}
    self.ts = ts
    self.history = []
    # Schema associated with the raw rows encapsulated within this Changelog
    self.internal_schema = schema
    # Columns in the flattened scan result of this changelog
    self.columns = [c['name'] for c in self.internal_schema['columns']]
    self.columns.insert(0, 'op')
    # Set for keeping track of all unique valid op codes received by this changelog.
    # This is used to determine whether or not it's a real changelog. Certain API
    # responses (such as for SHOW TABLES) will not include op codes in the result.
    # However, it's convenient to treat all server responses as changelogs rather than
    # special case them, so if we have a changelog that's not a real changelog, this
    # set allows us to identify that scenario and just scan it as a table instead.
    self.ops_received = set()

  def __iter__(self):
    return (r for r in self.history)

  def _has(self, property, meta=False):
    if not meta and self.properties.get('_meta'):
      return False
    return self.properties.get(property)

  def is_live(self):
    # Certain properties attached to a SET statement's changelog should
    # not be applied to the result of the SET itself. For example, if the
    # caller issues a SET 'flinklab.live' statement, the result of that
    # SET statement will include the 'flinklab.live' property, but obviously
    # this property is intended to take effect for future statemeents.
    return self._has('flinklab.live')

  def is_step(self):
    return self._has('flinklab.step')

  def is_file(self):
    return self._has('flinklab.output.path')

  def is_table(self):
    changelog = self.properties.get('flinklab.changelog')
    # We default to table mode (regardless of flinklab.results.mode) if
    # this changelog never received any valid op codes
    return not changelog or not self.ops_received

  def collapse(self):
    columns = [c['name'] for c in self.internal_schema['columns']]
    result = Table(columns)
    result.update(self.history)

    return result

  def dump(self):
    formatters = {
      'csv': dump_delimited,
      'json': dump_json,
      'tsv': lambda p, c, r: dump_delimited(p, c, r, delimiter='\t'),
    }
    path = self.properties.get('flinklab.output.path')
    if not path:
      raise RuntimeError('flinklab.output.path not set for this changelog')
    if os.path.exists(path):
      raise RuntimeError('%s already exists' % path)
    _, ext = os.path.splitext(path)
    # splitext() includes the '.' in the extension :/
    ext = ext[1:]
    if ext not in formatters:
      raise RuntimeError('output file extension must be in %s' % list(formatters.keys()))
    columns, rows = self.scan()

    return formatters[ext](path, columns, rows)

  def scan(self, copy=False):
    # Used by downstream consumers of this changelog, such as ResultsConsole.
    # Returns either a table or a changelog with op column depending on
    # current flinklab.result.mode. Note that this changelog may be scanned
    # using different result formats any number of times, as the changelog's
    # current history is kept in memory.
    if self.is_table():
      table = self.collapse()
      # Obviously we don't technically need to return table.columnns here since
      # its contained within the table, but we do anyways so that callers don't
      # need to do any special casing depending on what's returned by scan().
      return table.columns, table
    else:
      rows = (r for r in self.history)
      # If the caller wants a copy, copy the rows into memory and return
      # a reference to that. This can be necessary when multiple threads
      # are operating over the same Changelog concurrently. For example,
      # if one thread is consuming rows and sending them to another
      # thread for processing, and the processing thread has a generator
      # as above, the underlying list may change before it gets a chance
      # to process the original rows.
      #
      # In this case the generator is no longer an accurate representation of
      # what the processing thread thinks it has. We deal with this by simply
      # copying the result here. This only really comes into play when consuming
      # live output, in which case consumption will be happening in smaller
      # increments, so copying in this scenario is reasonable.
      if copy:
        rows = list(rows)
      return self.columns, rows

  def validate(self, data):
    op_labels = {
      0: '+I',
      1: '-U',
      2: '+U',
      3: '-D'
    }
    op = data.get('op')
    row = list(data['row'])
    # Note that we're validating incoming rows against the schema this
    # changelog was built from, not the set of columns in this changelog's
    # output (self.columns)
    if len(row) != len(self.internal_schema['columns']):
      args = (
        len(self.internal_schema['columns']),
        len(row),
        row
      )
      raise ValueError('table has %d columns but row has %d: %s' % args)
    if op is not None and op not in op_labels:
      raise ValueError('invalid op code received for row: %s' % data)

    # From this point forth, work with slightly more human friendly op codes
    if op is not None:
      op = op_labels[op]

    # Also note that we'll be working with flattened changelog rows now,
    # in which the op code is just the first column in each row
    return op, tuple([op] + list(row))

  def consume(self, limit=None, copy=False):
    limit = limit if limit is not None else sys.maxsize
    # Changelogs always wrap rows in a generator, which make this easy.
    # Each call to consume will pick up where the last one left off in
    # the generator, so no need to keep our own cursor.
    start = len(self.history)
    consumed = 0
    while consumed < limit:
      try:
        data = next(self.rows)
        op, row = self.validate(data)
        self.history.append(row)
        if op:
          self.ops_received.add(op)
        consumed += 1
      except StopIteration:
        break
    # Return the newly added changelog rows in the form of a slice
    # to avoid extraneous memory consumption, unless caller wants
    # a copy (generally for thread safety)
    result = self.history[start:]
    if copy:
      result = list(result)
    return result


# Basic Flink client for direct API calls to Flink, Schema Registry, and Kafka
class FlinkClient(object):

  def __init__(self, conf):
    auth = AuthEndpoint(conf)
    self.conf = conf
    self.statements = StatementsEndpoint(auth, conf)
    # Generic state dictionary to associated with this instance. This state is
    # currently used to track statements marked for deletion by 'flinklab.delete'.
    self.state = {}

  @property
  def catalog(self):
    return self.conf['flink']['sql.current-catalog']

  @property
  def database(self):
    return self.conf['flink']['sql.current-database']

  @functools.cache
  def get_sr_client(self, catalog):
    if catalog not in self.conf:
      raise KeyError('catalog %s not found in configuration file' % catalog)

    section = self.conf[catalog]
    return SchemaRegistryClient({
      'url': section['schema_registry']['endpoint'],
      'basic.auth.user.info': '%s:%s' % (
        section['schema_registry']['key'],
        section['schema_registry']['secret']
      )
    })

  @functools.cache
  def get_admin_client(self, catalog, database):
    if catalog not in self.conf:
      raise KeyError('catalog %s not found in configuration file' % catalog)
    if database not in self.conf[catalog]:
      raise KeyError('database %s not found under catalog %s' % (database, catalog))

    section = self.conf[catalog][database]
    return AdminClient({
      'sasl.mechanisms': 'PLAIN',
      'security.protocol': 'SASL_SSL',
      'bootstrap.servers': section['endpoint'],
      'sasl.username': section['key'],
      'sasl.password': section['secret']
    })

  def drop_table(self, catalog, database, table):
    dropped = []
    # Flink does not support DROP TABLE yet, so we simulate it by deleting
    # the target table's underlying topic and Schema Registry schema
    admin = self.get_admin_client(catalog, database)
    sr = self.get_sr_client(catalog)
    # Note that we don't need catalog and database anymore, since this admin client
    # is already connected to the target Kafka cluster
    result = admin.delete_topics([table])
    result[table].result()
    dropped.append(table)
    log.debug('deleted topic %s.%s', database, table)
    try:
      key = '%s-key' % table
      value = '%s-value' % table
      subjects = [s for s in sr.get_subjects() if s in (key, value)]
      for subject in subjects:
        sr.delete_subject(subject, permanent=True)
        dropped.append(subject)
        log.debug(' deleted subject %s', subject)
      return dropped
    except SchemaRegistryError as sre:
      log.warning(sre)


def rows_expected(stmt):
  return 'result_schema' in stmt['status']


def wrap_detail_as_result(detail):
  # Dummy schema we use when we want to wrap things like error
  # messages in a way that enables us to display them downstream
  # without special handling
  detail_schema = {
      'columns': [
        {'name': 'detail'}
      ]
    }
  row = {
    'row': [detail]
  }
  # Now put it into a single-row result set
  return detail_schema, [row]


def sanitize_properties(props):
  return dict([(k, v) for k, v in props.items() if 'flinklab.' not in k])


# Flink SQL interpreter for executing SqlScripts.
# Inherits the basic FlinkClient and interprets SQL scripts by translating
# them into the appropriate API calls.
#
# Note that this class doesn't necessarily need to be used exclusively for
# actual .sql files (although it can). It is useful to wrap even trivial
# in-memory SQL statements with the SqlScript class in order to provide more
# structure around statements, such as keeping SET properties in context
# across statement executions.
#
# The primary entrypoint for this functionality is the execute() method.
class FlinkSqlInterpreter(FlinkClient):

  properties_schema = {
    'columns': [
      {'name': 'property'},
      {'name': 'value'}
    ]
  }

  def handle_set(self, script, stmt):
    # Any properties SET in this script will stay in context until
    # we're done running it
    if not isinstance(stmt, tuple):
      # It's a RESET, which is represented as a SET with no value
      k = stmt
      if k is None:
        # It's just a bare RESET, clear all properties
        script.properties.clear()
      else:
        script.properties.pop(k, None)
      # Dump all current properties after resetting anything. Obviously
      # the script-level properties may have just been completely cleared,
      # but there may still be client-level properties in context.
      return self.handle_show(script, None)

    # SET param = value
    k, v = stmt
    script.properties[k] = v
    # Show the caller which property they SET
    changed = (k, str(v).lower())
    # Hint to the downstream receiver that this changelog should
    # be printed as a table
    props = script.properties.copy()
    props.setdefault('flinklab.changelog', False)
    # We do not always want to apply certain properties that were
    # set via SET to the below changelog (such as 'flinklab.live', which
    # tells console to display contnuous results). This tag enables us to
    # determine that this changelog belongs to a meta statement, and
    # therefore certain properties should be disregarded.
    props['_meta'] = True
    rows = [{'row': changed}]

    return Changelog(
      self.properties_schema, rows,
      ts=time.time(), properties=props
    )

  def handle_show(self, script, stmt):
    # Consolidate all in-context properties into one result set
    props = self.statements.default_properties.copy()
    # Script properties take precedence over any defaults
    props.update(script.properties)

    rows = []
    for k, v in props.items():
      if not stmt or k == stmt:
        rows.append({
          'row': [k, str(v).lower()]
        })

    # Hint to the downstream receiver that this changelog should
    # be printed as a table
    props.setdefault('flinklab.changelog', False)
    props['_meta'] = True

    return Changelog(
      self.properties_schema, rows,
      ts=time.time(), properties=props
    )

  def handle_drop(self, script, stmt):
    # DROP TABLE is unsupported by Flink, delete topic/schema manually
    target = stmt.split('.')
    # SET takes precedence over configured default catalog and database
    catalog = script.properties.get('sql.current-catalog', self.catalog)
    database = script.properties.get('sql.current-database', self.database)
    table = None

    # Unpack catalog, database and table depending on how many qualifiers we have
    assert len(target) <= 3, len(target)
    if len(target) == 1:
      table = target[0]
    elif len(target) == 2:
      database, table = target
    elif len(target) == 3:
      catalog, database, table = target

    start_time = time.time()
    # Now "drop" it
    self.drop_table(catalog, database, table)
    # We succeeded, fake a statement-style result set for displaying downstream
    d = 'Table \'%s\' dropped' % table
    schema, rows = wrap_detail_as_result(d)

    props = script.properties.copy()
    props.setdefault('flinklab.changelog', False)

    rs = Changelog(
      schema, rows,
      properties=props,
      ts=start_time
    )

    return rs

  def handle_sql(self, script, stmt, status_hook=None):
    start_time = time.time()
    stmt = self.statements.create(stmt, sanitize_properties(script.properties))
    if status_hook:
      status_hook(stmt)
    # SET 'flinklab.delete' can be used in scripts to tell flinklab
    # to delete any statements created while this property is set.
    # If this property is set, keep the statement so we can delete it
    # when we're done executing the script.
    if script.properties.get('flinklab.delete'):
      self.state[stmt['name']] = stmt

    ready = self.statements.wait_for_status(stmt, 'running', 'completed')
    if status_hook:
      status_hook(stmt)
    if ready is None:
      # Note that wait_for_status will update our copy of the statement
      # we gave it with the latest metadata, so it should have the error
      # assoicated with the failure
      detail = stmt['status']['detail']
      raise RuntimeError(detail)

    result_set = None
    props = script.properties.copy()

    if rows_expected(ready):
      # We expect to get rows back, wrap them in a generator for downstream consumption
      r = self.statements.results(ready['name'])
      schema = ready['status']['result_schema']
      result_set = Changelog(
        schema, r, statement=ready,
        properties=props,
        ts=start_time
      )
    else:
      # Statements such as CREATE TABLE will complete but do not return rows.
      # Make such responses appear like normal result sets so that they can
      # be processed and displayed downstream with no further special handling.
      d = ready['status']['detail'] or ready['status']['phase']
      schema, rows = wrap_detail_as_result(d)

      props.setdefault('flinklab.changelog', False)
      result_set = Changelog(
        schema, rows, statement=ready,
        properties=props,
        ts=start_time
      )

    event = {
      'name': ready['name'],
      'statement': ready,
      'properties': result_set.properties
    }
    log.debug('added %s result set', ready['name'], extra=event)

    return result_set

  def execute(self, sql, status_hook=None, begin_hook=None):
    # For convenience, this method allows passing raw SQL strings rather
    # than a SqlScript instance. Whenever this is the case, just wrap the
    # raw SQL in a SqlScript.
    if isinstance(sql, str):
      sql = SqlScript(raw=sql)
    # We generally work with SqlScripts instead of raw SQL strings, even
    # if they are small in-memory "scripts". SqlScripts add convenience around
    # managing context assigned via SET, and provide slightly more structure than
    # just raw SQL strings, which is useful for debugging and testing.
    #
    # The parsed script will contain a list of all distinct parsed statements.
    # Each of these statements is tagged with an operation. Certain statement
    # types are not supported by Flink yet, so we handle these separately by
    # looking at their operation tag.
    handle_sql = self.handle_sql
    if status_hook:
      def hooked(script, stmt):
        return self.handle_sql(script, stmt, status_hook=status_hook)
      handle_sql = hooked

    handlers = {
      'set': self.handle_set,
      'show': self.handle_show,
      'drop': self.handle_drop,
      'sql': handle_sql,
    }
    result_sets = sql.execute(handlers, begin_hook=begin_hook)
    # Each call to handle_sql will have returned a Changelog wrapping each
    # statement's results. Each result set is a generator, so result sets are
    # not consumed in memory before returning to the caller. Callers are free
    # to do this if they want to, but it's up to them.
    #
    # SqlScript.execute wraps these Changelogs in a generator, and this method returns
    # that generator. This gives the caller flexibiliy around how to run statements and
    # view results:
    #
    # Consuming this generator of Changelogs (e.g. by wrapping it in a list), but not
    # consuming the rows in each result set will cause all statements to execute before
    # rendering any output. To consume output in this scenario, the caller just needs to
    # iterate over the result sets again while consuming the rows they contain.
    #
    # Iterating over this generator and consuming each corresponding result set
    # per iteration will cause statements to be executed synchronously, consuming
    # a statement's output before executing the next statement.
    return result_sets

  def cleanup(self):
    # The script's state dictionary will contain statements (keyed on name)
    # created when flinklab.delete was set to True. Now that we're done with
    # everything, delete these statements.
    #
    # TODO(derekjn) make this state more compartmentalized. We will probably
    # want to do different things with different state in the future, rather
    # than just assume it's all statement names designated for deletion.
    if not self.state:
      return
    for name in self.state.keys():
      if not self.statements.delete(name):
        log.warning('statement %s does not exist', name)
      else:
        log.debug('deleted statement %s', name)
    self.state.clear()